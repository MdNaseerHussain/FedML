{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated ML\n",
    "## Contents\n",
    "1. Introduction\n",
    "1. Reading an image from the dataset\n",
    "1. Loading Images and Labels\n",
    "1. Splitting training and testing data\n",
    "1. Creating clients\n",
    "1. Creating helper nodes\n",
    "1. Batching clients' and test data\n",
    "1. Creating an MLP model\n",
    "1. Optimizer, Loss function and Metrics to compile the model\n",
    "1. Utility functions for the Federated Averaging\n",
    "1. Functions to fetch and set shape of model weights\n",
    "1. Modulation and Demodulation functions for Communication\n",
    "1. Transmission functions from Client to Helper and Helper to Master\n",
    "1. Model test function\n",
    "1. Federated Averaging Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we have implemented a working Federated ML model. The goal is to simulate packet losses in communication between the server and the clients in the Federated Averaging Algorithm. The function once implemented will be added to the algorithm near the TODO's in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading an image from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "image = cv2.imread(\"datasets/numbers/trainingSet/0/img_1.jpg\")\n",
    "plt.imshow(image)\n",
    "plt.title(\"Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "dir_path = \"datasets/numbers/trainingSet/\"\n",
    "images = list()\n",
    "labels = list()\n",
    "for number in range(0, 10):\n",
    "    folder = dir_path + str(number)\n",
    "    for image_file in os.listdir(folder):\n",
    "        image_gray = cv2.imread(os.path.join(folder, image_file), cv2.IMREAD_GRAYSCALE)\n",
    "        image = np.array(image_gray).flatten()\n",
    "        images.append(image/255)\n",
    "        labels.append(number)\n",
    "    print(\"[INFO] Processed {}/{}\".format(number + 1, 10))\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "images_train, images_test, labels_train, labels_test = train_test_split(images, labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_clients = 16\n",
    "client_names = [\"client_{}\".format(i + 1) for i in range(num_clients)]\n",
    "data = list(zip(images, labels))\n",
    "random.shuffle(data)\n",
    "size = len(data)//num_clients\n",
    "data_shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "clients = {client_names[i]: data_shards[i] for i in range(num_clients)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating helper nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_helpers = math.ceil(math.log2(num_clients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching clients' and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "batch_size = 32\n",
    "clients_batched = dict()\n",
    "for (client_name, data_shard) in clients.items():\n",
    "    data, labels = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(labels)))\n",
    "    clients_batched[client_name] = dataset.shuffle(len(labels)).batch(batch_size)\n",
    "test_batched = tf.data.Dataset.from_tensor_slices((images_test, labels_test)).batch(len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Multi Layer Perception (MLP) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(200, input_shape=(shape,)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer, Loss function and Metrics to compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "lr = 0.01 \n",
    "comms_round = 100\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(lr=lr, decay=lr/comms_round, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions for the Federated Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scaling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    avg_grad = list()\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "    return avg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to fetch and set shape of model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(weights):\n",
    "    shapes = [i.shape for i in weights]\n",
    "    return shapes\n",
    "\n",
    "def flatten_model_weights(weights):\n",
    "    flattened_weights = np.concatenate([i.flatten() for i in weights])\n",
    "    return flattened_weights\n",
    "\n",
    "def restore_model_shape(flattened_weights, shapes):\n",
    "    weights = []\n",
    "    index = 0\n",
    "    for shape in shapes:\n",
    "        size = np.product(shape)\n",
    "        arr = np.array(flattened_weights[index : index + size])\n",
    "        weights.append(arr.reshape(shape))\n",
    "        index += size\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulation and Demodulation Functions for Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitstring import BitArray\n",
    "\n",
    "def modulation(weights):\n",
    "    weights = flatten_model_weights(weights)\n",
    "    packets = []\n",
    "    for i in range(len(weights)):\n",
    "        packet = BitArray(float=weights[i], length=32)\n",
    "        packets.append(packet.bin)\n",
    "    return packets\n",
    "\n",
    "def demodulation(weights, model_shape):\n",
    "    return restore_model_shape(weights, model_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transmission functions from Client to Helper and Helper to Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def calculateDecodingProbability(n, q):\n",
    "    if n < 3:\n",
    "        return 0\n",
    "    k = 3\n",
    "    num, den = 1, 1\n",
    "    for i in range(0, k):\n",
    "        num *= (1 - q**(i - n))\n",
    "        den *= (1 - q**(i - n + 1))\n",
    "    return 1 - (1 - num)/(1 - den)\n",
    "\n",
    "def transmissionCH(packet_loss_prob, q):\n",
    "    succesfully_transmitted = 0\n",
    "    transmissions = []\n",
    "    transmissionCnt = 0\n",
    "    for i in range(4):\n",
    "        transmissionCnt += 1\n",
    "        if random.random() > packet_loss_prob:\n",
    "            transmissions.append(1)\n",
    "            succesfully_transmitted += 1\n",
    "            probability_success = calculateDecodingProbability(succesfully_transmitted, q)\n",
    "            if random.random() < probability_success:\n",
    "                return transmissions, True\n",
    "        else:\n",
    "            transmissions.append(0)\n",
    "    return transmissions, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import CategoricalCrossentropy\n",
    "from sklearn.metrics import accuracy_score\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = CategoricalCrossentropy(from_logits=True)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round+1, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated Averaging Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def fedML(packet_loss_prob=0, q=2**8):\n",
    "    print(q)\n",
    "    smlp_global = SimpleMLP()\n",
    "    global_model = smlp_global.build(784, 10)\n",
    "    model_shape = get_shape(global_model.get_weights())\n",
    "    accuracy = []\n",
    "    for comm_round in range(comms_round):\n",
    "        global_weights = global_model.get_weights()\n",
    "        client_names= list(clients_batched.keys())\n",
    "        scaled_weights_list = []\n",
    "        comm_matrix = []\n",
    "        random.shuffle(client_names)\n",
    "        lost_clients = 0\n",
    "        for client_number, client in enumerate(client_names):\n",
    "            smlp_local = SimpleMLP()\n",
    "            local_model = smlp_local.build(784, 10)\n",
    "            local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "            local_model.set_weights(global_weights)\n",
    "            local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "            scaling_factor = weight_scaling_factor(clients_batched, client)\n",
    "            scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "            bitWeights = modulation(scaled_weights)\n",
    "            transmissions, decodable = transmissionCH(packet_loss_prob, q)\n",
    "            comm_matrix.append(transmissions)\n",
    "            if not decodable:\n",
    "                lost_clients += 1\n",
    "            else:\n",
    "                weights = bitWeights\n",
    "                for i in range(len(bitWeights)):\n",
    "                    weights[i] = BitArray(bin=bitWeights[i]).float\n",
    "                scaled_weights_list.append(weights)\n",
    "            K.clear_session()\n",
    "        if lost_clients != 0:\n",
    "            compensated_lost_weights = scale_model_weights(global_weights, lost_clients*weight_scaling_factor(clients_batched, 'client_1'))\n",
    "            packets = modulation(compensated_lost_weights)\n",
    "            weights = packets\n",
    "            for i in range(len(packets)):\n",
    "                weights[i] = BitArray(bin=packets[i]).float\n",
    "            scaled_weights_list.append(weights)\n",
    "        global_weights = sum_scaled_weights(scaled_weights_list)\n",
    "        global_weights = demodulation(global_weights, model_shape)\n",
    "        global_model.set_weights(global_weights)\n",
    "        with open('output/rlnc/Q{}/comm_matrix{}.txt'.format(q, 100*packet_loss_prob), 'a') as f:\n",
    "            f.write('Round ' + str(comm_round) + ': ' + str(comm_matrix) + '\\n')\n",
    "        for(X_test, Y_test) in test_batched:\n",
    "            global_acc, _ = test_model(X_test, Y_test, global_model, comm_round)\n",
    "            accuracy.append(global_acc)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = fedML(0.25, 4)\n",
    "np.savetxt('output/rlnc/Q{}/comm_matrix{}.txt'.format(4, 25), acc, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = fedML(0.25, 8)\n",
    "np.savetxt('output/rlnc/Q{}/comm_matrix{}.txt'.format(8, 25), acc, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = fedML(0.25, 16)\n",
    "np.savetxt('output/rlnc/Q{}/comm_matrix{}.txt'.format(16, 25), acc, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = fedML(0.25, 64)\n",
    "np.savetxt('output/rlnc/Q{}/comm_matrix{}.txt'.format(64, 25), acc, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = fedML(0.25, 256)\n",
    "np.savetxt('output/rlnc/Q{}/comm_matrix{}.txt'.format(256, 25), acc, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a35bf13b97148589d342a45d495f7a518789f6a66d3c54eecac9b61e53c58857"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
