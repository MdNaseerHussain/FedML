{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated ML\n",
    "## Contents\n",
    "1. Introduction\n",
    "1. Reading an image from the dataset\n",
    "1. Loading Images and Labels\n",
    "1. Splitting training and testing data\n",
    "1. Creating clients\n",
    "1. Creating helper nodes\n",
    "1. Batching clients' and test data\n",
    "1. Creating an MLP model\n",
    "1. Optimizer, Loss function and Metrics to compile the model\n",
    "1. Utility functions for the Federated Averaging\n",
    "1. Functions to fetch and set shape of model weights\n",
    "1. Modulation and Demodulation functions for Communication\n",
    "1. Transmission functions from Client to Helper and Helper to Master\n",
    "1. Model test function\n",
    "1. Federated Averaging Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we have implemented a working Federated ML model. The goal is to simulate packet losses in communication between the server and the clients in the Federated Averaging Algorithm. The function once implemented will be added to the algorithm near the TODO's in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading an image from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image')"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "image = cv2.imread(\"datasets/numbers/trainingSet/0/img_1.jpg\")\n",
    "plt.imshow(image)\n",
    "plt.title(\"Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Images and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processed 1/10\n",
      "[INFO] Processed 2/10\n",
      "[INFO] Processed 3/10\n",
      "[INFO] Processed 4/10\n",
      "[INFO] Processed 5/10\n",
      "[INFO] Processed 6/10\n",
      "[INFO] Processed 7/10\n",
      "[INFO] Processed 8/10\n",
      "[INFO] Processed 9/10\n",
      "[INFO] Processed 10/10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "dir_path = \"datasets/numbers/trainingSet/\"\n",
    "images = list()\n",
    "labels = list()\n",
    "for number in range(0, 10):\n",
    "    folder = dir_path + str(number)\n",
    "    for image_file in os.listdir(folder):\n",
    "        image_gray = cv2.imread(os.path.join(folder, image_file), cv2.IMREAD_GRAYSCALE)\n",
    "        image = np.array(image_gray).flatten()\n",
    "        images.append(image/255)\n",
    "        labels.append(number)\n",
    "    print(\"[INFO] Processed {}/{}\".format(number + 1, 10))\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "images_train, images_test, labels_train, labels_test = train_test_split(images, labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "num_clients = 16\n",
    "client_names = [\"client_{}\".format(i + 1) for i in range(num_clients)]\n",
    "data = list(zip(images, labels))\n",
    "random.shuffle(data)\n",
    "size = len(data)//num_clients\n",
    "data_shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "clients = {client_names[i]: data_shards[i] for i in range(num_clients)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating helper nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_helpers = math.ceil(math.log2(num_clients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching clients' and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "batch_size = 32\n",
    "clients_batched = dict()\n",
    "for (client_name, data_shard) in clients.items():\n",
    "    data, labels = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(labels)))\n",
    "    clients_batched[client_name] = dataset.shuffle(len(labels)).batch(batch_size)\n",
    "test_batched = tf.data.Dataset.from_tensor_slices((images_test, labels_test)).batch(len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Multi Layer Perception (MLP) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(200, input_shape=(shape,)))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(200))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer, Loss function and Metrics to compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdnaj\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "lr = 0.01 \n",
    "comms_round = 100\n",
    "loss='categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(lr=lr, decay=lr/comms_round, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions for the Federated Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scaling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    avg_grad = list()\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "    return avg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to fetch and set shape of model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape(weights):\n",
    "    shapes = [i.shape for i in weights]\n",
    "    return shapes\n",
    "\n",
    "def flatten_model_weights(weights):\n",
    "    flattened_weights = np.concatenate([i.flatten() for i in weights])\n",
    "    return flattened_weights\n",
    "\n",
    "def restore_model_shape(flattened_weights, shapes):\n",
    "    weights = []\n",
    "    index = 0\n",
    "    for shape in shapes:\n",
    "        size = np.product(shape)\n",
    "        arr = np.array(flattened_weights[index : index + size])\n",
    "        weights.append(arr.reshape(shape))\n",
    "        index += size\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulation and Demodulation Functions for Communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitstring import BitArray\n",
    "\n",
    "def modulation(weights):\n",
    "    weights = flatten_model_weights(weights)\n",
    "    packets = []\n",
    "    for i in range(len(weights)):\n",
    "        packet = BitArray(float=weights[i], length=32)\n",
    "        packets.append(packet.bin)\n",
    "    return packets\n",
    "\n",
    "def demodulation(weights, model_shape):\n",
    "    return restore_model_shape(weights, model_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transmission functions from Client to Helper and Helper to Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "scaled_weights_list = []\n",
    "comm_matrix = []\n",
    "\n",
    "def create_comm_matrix(num_clients, num_helpers):\n",
    "    scaled_weights_list.clear()\n",
    "    comm_matrix.clear()\n",
    "    for i in range(num_clients):\n",
    "        comm_matrix.append([])\n",
    "        for j in range(num_helpers):\n",
    "            comm_matrix[i].append(0)\n",
    "    return comm_matrix\n",
    "\n",
    "def calculateDecodingProbability(n):\n",
    "    k, q = 3, 2**16\n",
    "    num, den = 1, 1\n",
    "    for i in range(0, k):\n",
    "        num *= (1 - q**(i - n))\n",
    "        den *= (1 - q**(i - n + 1))\n",
    "    return 1 - (1 - num)/(1 - den)\n",
    "\n",
    "def transmissionCH(C_ch, comm_matrix, client, packet_loss_prob):\n",
    "    succesfully_transmitted = 0\n",
    "    for i in range(len(comm_matrix[client])):\n",
    "        C_ch += 1\n",
    "        if random.random() > packet_loss_prob:\n",
    "            comm_matrix[client][i] = 1\n",
    "            succesfully_transmitted += 1\n",
    "    if succesfully_transmitted >= 3:\n",
    "        return comm_matrix, C_ch, True\n",
    "    while C_ch < 9:\n",
    "        C_ch += 1\n",
    "        if random.random() > packet_loss_prob:\n",
    "            comm_matrix[client].append(1)\n",
    "            succesfully_transmitted += 1\n",
    "        else:\n",
    "            comm_matrix[client].append(0)\n",
    "        probability_success = calculateDecodingProbability(succesfully_transmitted)\n",
    "        if random.random() < probability_success:\n",
    "            return comm_matrix, C_ch, True\n",
    "    return comm_matrix, C_ch, False\n",
    "    \n",
    "def transmissionHM(comm_matrix):\n",
    "    C_hm = 0\n",
    "    comm_links = {}\n",
    "    for i in range(len(comm_matrix)):\n",
    "        if comm_matrix[i].count(1) < 3:\n",
    "            continue\n",
    "        elif len(comm_matrix[i]) > 4:\n",
    "            C_hm += comm_matrix[i].count(1)\n",
    "        else:\n",
    "            mask = ''\n",
    "            for j in range(len(comm_matrix[i])):\n",
    "                mask += str(comm_matrix[i][j])\n",
    "            if mask in comm_links:\n",
    "                comm_links[mask].append(i)\n",
    "            else:\n",
    "                comm_links[mask] = [i]\n",
    "    for _ in comm_links:\n",
    "        C_hm += 3\n",
    "    return C_hm/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import CategoricalCrossentropy\n",
    "from sklearn.metrics import accuracy_score\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = CategoricalCrossentropy(from_logits=True)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round+1, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated Averaging Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def fedML(packet_loss_prob=0):\n",
    "    smlp_global = SimpleMLP()\n",
    "    global_model = smlp_global.build(784, 10)\n",
    "    model_shape = get_shape(global_model.get_weights())\n",
    "    C_CH, C_HM, accuracy = [], [], []\n",
    "    for comm_round in range(comms_round):\n",
    "        global_weights = global_model.get_weights()\n",
    "        client_names= list(clients_batched.keys())\n",
    "        comm_matrix = create_comm_matrix(num_clients, num_helpers)\n",
    "        C_ch_list, C_hm = [], 0\n",
    "        random.shuffle(client_names)\n",
    "        lost_clients = 0\n",
    "        for client_number, client in enumerate(client_names):\n",
    "            smlp_local = SimpleMLP()\n",
    "            local_model = smlp_local.build(784, 10)\n",
    "            local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "            local_model.set_weights(global_weights)\n",
    "            local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "            scaling_factor = weight_scaling_factor(clients_batched, client)\n",
    "            scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "            bitWeights = modulation(scaled_weights)\n",
    "            C_ch = 0\n",
    "            comm_matrix, C_ch, decodable = transmissionCH(C_ch, comm_matrix, client_number, packet_loss_prob)\n",
    "            C_ch_list.append(C_ch)\n",
    "            if not decodable:\n",
    "                lost_clients += 1\n",
    "            else:\n",
    "                weights = bitWeights\n",
    "                for i in range(len(bitWeights)):\n",
    "                    weights[i] = BitArray(bin=bitWeights[i]).float\n",
    "                scaled_weights_list.append(weights)\n",
    "            K.clear_session()\n",
    "        C_hm = transmissionHM(comm_matrix)\n",
    "        if lost_clients != 0:\n",
    "            compensated_lost_weights = scale_model_weights(global_weights, lost_clients*weight_scaling_factor(clients_batched, 'client_1'))\n",
    "            packets = modulation(compensated_lost_weights)\n",
    "            weights = packets\n",
    "            for i in range(len(packets)):\n",
    "                weights[i] = BitArray(bin=packets[i]).float\n",
    "            scaled_weights_list.append(weights)\n",
    "        global_weights = sum_scaled_weights(scaled_weights_list)\n",
    "        global_weights = demodulation(global_weights, model_shape)\n",
    "        global_model.set_weights(global_weights)\n",
    "        with open('output/rlnc/comm_matrix50.txt', 'a') as f:\n",
    "            f.write('Round ' + str(comm_round) + ': ' + str(comm_matrix) + '\\n')\n",
    "        for(X_test, Y_test) in test_batched:\n",
    "            global_acc, _ = test_model(X_test, Y_test, global_model, comm_round)\n",
    "            accuracy.append(global_acc)\n",
    "        C_CH.append(sum(C_ch_list) / num_clients)\n",
    "        C_HM.append(C_hm / num_helpers)\n",
    "    return accuracy, C_CH, C_HM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 1 | global_acc: 86.810% | global_loss: 1.7127728462219238\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 2 | global_acc: 89.310% | global_loss: 1.6424325704574585\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 3 | global_acc: 90.905% | global_loss: 1.615587592124939\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 4 | global_acc: 91.595% | global_loss: 1.6025757789611816\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 5 | global_acc: 92.262% | global_loss: 1.5927815437316895\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 6 | global_acc: 92.690% | global_loss: 1.58583664894104\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 7 | global_acc: 93.095% | global_loss: 1.5797816514968872\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 8 | global_acc: 93.286% | global_loss: 1.5750627517700195\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 9 | global_acc: 93.690% | global_loss: 1.5706417560577393\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 10 | global_acc: 93.857% | global_loss: 1.567311406135559\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 11 | global_acc: 94.000% | global_loss: 1.563880443572998\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 12 | global_acc: 94.262% | global_loss: 1.5612519979476929\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 13 | global_acc: 94.310% | global_loss: 1.5592219829559326\n",
      "132/132 [==============================] - 1s 3ms/step\n",
      "comm_round: 14 | global_acc: 94.500% | global_loss: 1.556892991065979\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 15 | global_acc: 94.690% | global_loss: 1.555323839187622\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 16 | global_acc: 94.643% | global_loss: 1.5532439947128296\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 17 | global_acc: 94.810% | global_loss: 1.5510578155517578\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 18 | global_acc: 94.929% | global_loss: 1.5494385957717896\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 19 | global_acc: 94.905% | global_loss: 1.5481994152069092\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 20 | global_acc: 95.000% | global_loss: 1.5468782186508179\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 21 | global_acc: 95.143% | global_loss: 1.545801043510437\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 22 | global_acc: 95.095% | global_loss: 1.5448702573776245\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 23 | global_acc: 95.119% | global_loss: 1.543449878692627\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 24 | global_acc: 95.405% | global_loss: 1.5420689582824707\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 25 | global_acc: 95.381% | global_loss: 1.5411521196365356\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 26 | global_acc: 95.333% | global_loss: 1.54059636592865\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 27 | global_acc: 95.571% | global_loss: 1.5391268730163574\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 28 | global_acc: 95.619% | global_loss: 1.5384479761123657\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 29 | global_acc: 95.690% | global_loss: 1.537400722503662\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 30 | global_acc: 95.738% | global_loss: 1.5367238521575928\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 31 | global_acc: 95.810% | global_loss: 1.5358983278274536\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 32 | global_acc: 95.786% | global_loss: 1.535220742225647\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 33 | global_acc: 95.833% | global_loss: 1.5349198579788208\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 34 | global_acc: 95.857% | global_loss: 1.5341302156448364\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 35 | global_acc: 95.929% | global_loss: 1.5335419178009033\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 36 | global_acc: 95.976% | global_loss: 1.5328031778335571\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 37 | global_acc: 96.024% | global_loss: 1.5320589542388916\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 38 | global_acc: 96.071% | global_loss: 1.531399130821228\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 39 | global_acc: 96.024% | global_loss: 1.5314464569091797\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 40 | global_acc: 96.071% | global_loss: 1.5306843519210815\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 41 | global_acc: 96.262% | global_loss: 1.53032386302948\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 42 | global_acc: 96.214% | global_loss: 1.5299502611160278\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 43 | global_acc: 96.310% | global_loss: 1.529133915901184\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 44 | global_acc: 96.310% | global_loss: 1.528882622718811\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 45 | global_acc: 96.310% | global_loss: 1.5282894372940063\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 46 | global_acc: 96.310% | global_loss: 1.5278418064117432\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 47 | global_acc: 96.452% | global_loss: 1.5272754430770874\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 48 | global_acc: 96.476% | global_loss: 1.526940107345581\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 49 | global_acc: 96.381% | global_loss: 1.5267263650894165\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 50 | global_acc: 96.405% | global_loss: 1.5263195037841797\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 51 | global_acc: 96.476% | global_loss: 1.5259891748428345\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 52 | global_acc: 96.571% | global_loss: 1.5256389379501343\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 53 | global_acc: 96.524% | global_loss: 1.5252587795257568\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 54 | global_acc: 96.500% | global_loss: 1.524929165840149\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 55 | global_acc: 96.524% | global_loss: 1.52471923828125\n",
      "132/132 [==============================] - 0s 2ms/step\n",
      "comm_round: 56 | global_acc: 96.619% | global_loss: 1.5241070985794067\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 57 | global_acc: 96.667% | global_loss: 1.523956298828125\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 58 | global_acc: 96.595% | global_loss: 1.523806095123291\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 59 | global_acc: 96.667% | global_loss: 1.5234216451644897\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 60 | global_acc: 96.690% | global_loss: 1.5229583978652954\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 61 | global_acc: 96.571% | global_loss: 1.5228482484817505\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 62 | global_acc: 96.667% | global_loss: 1.5225379467010498\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 63 | global_acc: 96.714% | global_loss: 1.522233009338379\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 64 | global_acc: 96.690% | global_loss: 1.5222461223602295\n",
      "132/132 [==============================] - 1s 6ms/step\n",
      "comm_round: 65 | global_acc: 96.786% | global_loss: 1.5216047763824463\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 66 | global_acc: 96.786% | global_loss: 1.5214143991470337\n",
      "132/132 [==============================] - 1s 6ms/step\n",
      "comm_round: 67 | global_acc: 96.857% | global_loss: 1.5212935209274292\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 68 | global_acc: 96.762% | global_loss: 1.5210574865341187\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 69 | global_acc: 96.714% | global_loss: 1.5207161903381348\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 70 | global_acc: 96.810% | global_loss: 1.5203404426574707\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 71 | global_acc: 96.881% | global_loss: 1.5202010869979858\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 72 | global_acc: 96.881% | global_loss: 1.5200316905975342\n",
      "132/132 [==============================] - 1s 6ms/step\n",
      "comm_round: 73 | global_acc: 96.833% | global_loss: 1.5199187994003296\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 74 | global_acc: 96.905% | global_loss: 1.5195913314819336\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 75 | global_acc: 96.881% | global_loss: 1.5193920135498047\n",
      "132/132 [==============================] - 1s 6ms/step\n",
      "comm_round: 76 | global_acc: 96.952% | global_loss: 1.519118309020996\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 77 | global_acc: 96.905% | global_loss: 1.5189541578292847\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 78 | global_acc: 96.881% | global_loss: 1.518792748451233\n",
      "132/132 [==============================] - 1s 3ms/step\n",
      "comm_round: 79 | global_acc: 96.929% | global_loss: 1.518736720085144\n",
      "132/132 [==============================] - 1s 3ms/step\n",
      "comm_round: 80 | global_acc: 96.929% | global_loss: 1.5184504985809326\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 81 | global_acc: 96.905% | global_loss: 1.518364667892456\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 82 | global_acc: 97.024% | global_loss: 1.5179717540740967\n",
      "132/132 [==============================] - 1s 3ms/step\n",
      "comm_round: 83 | global_acc: 97.048% | global_loss: 1.5178617238998413\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 84 | global_acc: 97.024% | global_loss: 1.51771879196167\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 85 | global_acc: 96.976% | global_loss: 1.5175830125808716\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 86 | global_acc: 96.976% | global_loss: 1.5173929929733276\n",
      "132/132 [==============================] - 1s 3ms/step\n",
      "comm_round: 87 | global_acc: 97.071% | global_loss: 1.5172635316848755\n",
      "132/132 [==============================] - 1s 3ms/step\n",
      "comm_round: 88 | global_acc: 97.071% | global_loss: 1.517035961151123\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 89 | global_acc: 97.024% | global_loss: 1.5168964862823486\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 90 | global_acc: 97.071% | global_loss: 1.51667320728302\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 91 | global_acc: 97.071% | global_loss: 1.5165961980819702\n",
      "132/132 [==============================] - 1s 8ms/step\n",
      "comm_round: 92 | global_acc: 97.119% | global_loss: 1.5163960456848145\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 93 | global_acc: 97.048% | global_loss: 1.5161938667297363\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "comm_round: 94 | global_acc: 97.095% | global_loss: 1.5160787105560303\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 95 | global_acc: 97.119% | global_loss: 1.5159116983413696\n",
      "132/132 [==============================] - 0s 3ms/step\n",
      "comm_round: 96 | global_acc: 97.119% | global_loss: 1.5158436298370361\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 97 | global_acc: 97.119% | global_loss: 1.51564359664917\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 98 | global_acc: 97.119% | global_loss: 1.5155234336853027\n",
      "132/132 [==============================] - 1s 5ms/step\n",
      "comm_round: 99 | global_acc: 97.071% | global_loss: 1.5154322385787964\n",
      "132/132 [==============================] - 1s 3ms/step\n",
      "comm_round: 100 | global_acc: 97.095% | global_loss: 1.5151335000991821\n"
     ]
    }
   ],
   "source": [
    "# accuracies0, C_CH0, C_HM0 = fedML(0)\n",
    "# accuracies10, C_CH10, C_HM10 = fedML(0.1)\n",
    "# accuracies25, C_CH25, C_HM25 = fedML(0.25)\n",
    "accuracies50, C_CH50, C_HM50 = fedML(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('./output/rlnc/acc50.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for i in range(len(accuracies50)):\n",
    "        writer.writerow(accuracies50[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a35bf13b97148589d342a45d495f7a518789f6a66d3c54eecac9b61e53c58857"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
